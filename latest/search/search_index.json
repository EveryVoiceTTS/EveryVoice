{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#background","title":"Background","text":"<p>There are approximately 70 Indigenous languages spoken in Canada from 10 distinct language families. As a consequence of the residential school system and other policies of cultural suppression, the majority of these languages now have fewer than 500 fluent speakers remaining, most of them elderly.</p> <p>Despite this, Indigenous people have resisted colonial policies and continued speaking their languages, with interest by students and parents in Indigenous language education continuing to grow. Teachers are often overwhelmed by the number of students, and the trend towards online education means many students who have not previously had access to language classes now do. Supporting these growing cohorts of students comes with unique challenges in languages with few fluent first-language speakers. Teachers are particularly concerned with providing their students with opportunities to hear the language outside of class.</p> <p>While there is no replacement for a speaker of an Indigenous language, there are possible applications for speech synthesis (text-to-speech) to supplement existing text-based tools like verb conjugators, dictionaries and phrasebooks.</p> <p>The National Research Council has partnered with the Onkwawenna Kentyohkwa Kanyen\u2019k\u00e9ha immersion school, W\u0331S\u00c1NE\u0106 School Board, University nuhelot\u2019\u012fne thaiyots\u2019\u012f nistameyim\u00e2kanak Blue Quills, and the University of Edinburgh to research and develop state-of-the-art speech synthesis (text-to-speech) systems and techniques for Indigenous languages in Canada, with a focus on how to integrate text-to-speech technology into the classroom.</p> <p>The project is titled Speech Generation for Indigenous Language Education (SGILE).</p> <p>More information and motivation for this project can be found in our ACL2022 paper</p>"},{"location":"#what-is-everyvoice-and-this-documentation-about","title":"What is EveryVoice and this documentation about?","text":"<p>This project is the location of active research and development for the Speech Generation for Indigenous Language Education project. In addition to being a model for this project, it is meant to outline repeatable recipes for other communities and languages to develop their own text-to-speech systems. This documentation describes guides for how to do this.</p> <p>Note</p> <p>We are trying to develop a tool that makes the developer experience as smooth as possible. But, building these models and creating your datasets can be complicated. We recommend you are comfortable with Python and using the command line before starting on this project.</p>"},{"location":"#similar-projects-exist-why-create-another-one","title":"Similar projects exist, why create another one?","text":"<p>It is true that similar excellent projects exist, such as ESPnet, \ud83d\udc38TTS, Comprehensive-Transformer-TTS, and IMS-Toucan among others. Our reasons for creating our own are multi-fold (some of the following features are implemented in the aforementioned projects, but not every one of these features is supported in any of them):</p> <ul> <li>We support digraph/trigraph/multigraph inputs by defining our symbol sets as lists of strings instead of strings and using a custom tokenizer. This means we expect that your language might have inputs that consist of more than one symbol, like \"kw\" or \"tl\".</li> <li>We support out-of-the-box integration with g2p which allows the g2p rules for 30+ Indigenous languages to be used in the project.</li> <li>We implement the use of multi-hot phonological feature vector inputs for easier pre-training/fine-tuning. We implement this using the panphon library.</li> <li>We will not try to implement many different models. Instead we will curate a selection of models that we believe to be best for training models on under-resourced languages. In this way we are more similar to IMS-Toucan than ESPnet</li> <li>We use a custom, statically-typed configuration architecture between models written in Pydantic that allows for configuration validation and serialization/de-serialization to json and yaml. It also allows us to ensure the same configuration for text and audio processing is used between models.</li> <li>We implement our models in PyTorch Lightning</li> <li>We implement separate and joint training of our feature prediction and vocoder models</li> </ul> <p>Note</p> <p>These features do not necessarily mean that this is the right project for you. The other projects mentioned are of very high quality and might be a better fit for your project, particularly if you are lucky enough to have lots of data, or a language that is already supported.</p>"},{"location":"install/","title":"Installation","text":"<p>In order to train on GPUs, you must install PyTorch and Cuda. To do so, we recommend:</p> <ul> <li>installing conda or miniconda</li> <li>creating a new environment: <code>conda create --name EveryVoice python=3.9</code></li> <li>activating the environment: <code>conda activate EveryVoice</code></li> <li>following the PyTorch installation instructions relevant to your hardware</li> </ul> <p>We then recommend using an interactive installation after cloning the repo from GitHub:</p> <pre><code>$ git clone https://github.com/roedoejet/EveryVoice.git\n$ cd EveryVoice\n$ git submodule update --init\n$ conda activate EveryVoice\n$ pip install -e .\n</code></pre>"},{"location":"guides/","title":"Guides","text":"<p>Here are a selection of guides to help you through the process of training and using your own text-to-speech models.</p> <ol> <li> <p>Background to TTS</p> </li> <li> <p>Custom</p> </li> </ol>"},{"location":"guides/background/","title":"Background to Text-to-Speech","text":""},{"location":"guides/custom/","title":"Customize to your language","text":""},{"location":"guides/custom/#step-1-make-sure-you-have-permission","title":"Step 1: Make sure you have Permission!","text":"<p>So, you want to build a text-to-speech system for a new language or dataset - cool! But, just because you can build a text-to-speech system, doesn't mean you should. There are a lot of tricky ethical questions around text-to-speech. It's not ethical to just use audio you find somewhere if it doesn't have explicit permission to use it for the purposes of text-to-speech. The first step is to make sure you have permission to use the data in question and that whoever contributed their voice to the data you want to use is aware and supportive of your goal.</p>"},{"location":"guides/custom/#step-2-gather-your-data","title":"Step 2: Gather Your Data","text":"<p>The first thing to do is to get all the data you have (in this case audio with text transcripts) together in one place. Your audio should be in 'wav' format. Ideally it would be 16bit, mono (one channel) audio sampled somewhere between 22.05kHz and 48kHz. If that doesn't mean anything to you, don't worry, we can ensure the right format in later steps. It's best if your audio clips are somewhere between half a second and 10 seconds long. Any longer and it could be difficult to train. If your audio is longer than this, we suggest processing it into smaller chunks first.</p> <p>Your text should be consistently written and should be in a pipe-separated values spreadsheet, similar to this file. It should have a column that contains text and a column that contains the <code>basename</code> of your associated audio file. So if you have a recording of somebody saying \"hello how are you?\" and the corresponding audio is called <code>mydata0001.wav</code> then you should have a psv file that looks like this:</p> <pre><code>basename|text\nmydata0001|hello how are you?\nmydata0002|some other sentence.\n...\n</code></pre> <p>We also support comma and tab separated files, but recommend using pipes (|).</p> <p>You can also use the \"festival\" format which is like this (example from Sinhala TTS):</p> <pre><code>( sin_2241_0329430812 \" \u0d9a\u0ddd\u0d9a\u0da7\u0dad\u0dca \u0db8\u0d82 \u0dc0\u0dd9\u0db1\u0daf\u0dcf \u0dad\u0dbb\u0db8\u0dca \u0d9a\u0dcf\u0dbd\u0dd9 \u0d9c\u0db1\u0dca\u0db1\u0dd0\u0dad\u0dd2\u0dc0 \u0d87\u0db3 \u0d9c\u0dad\u0dca\u0dad\u0dcf \" )\n( sin_2241_0598895166 \" \u0d87\u0db1\u0dca\u0da2\u0dbd\u0dd3\u0db1\u0dcf \u0da2\u0ddc\u0dbd\u0dd3 \u0d9a\u0dd2\u0dba\u0db1\u0dca\u0db1\u0dda \u0db4\u0dc3\u0dd4\u0d9c\u0dd2\u0dba \u0daf\u0dd2\u0db1\u0dc0\u0dbd \u0db6\u0ddc\u0dc4\u0ddd \u0dc3\u0dd9\u0dba\u0dd2\u0db1\u0dca \u0d9a\u0dad\u0dcf \u0db6\u0dc4\u0da7 \u0dbd\u0d9a\u0dca\u0dc0\u0dd6 \u0da0\u0dbb\u0dd2\u0dad\u0dba\u0d9a\u0dca \" )\n( sin_2241_0701577369 \" \u0d86\u0dbb\u0dca\u0dae\u0dd2\u0d9a \u0da0\u0dd2\u0db1\u0dca\u0dad\u0db1\u0dba \u0dc4\u0dcf \u0dc3\u0dcf\u0db8\u0dcf\u0da2\u0dd3\u0dba \u0daf\u0dd2\u0dba\u0dd4\u0dab\u0dd4\u0dc0 \u0d87\u0dad\u0dd2 \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dc0\u0db1\u0dd4\u0dba\u0dda \u0db4\u0dd4\u0daf\u0dca\u0d9c\u0dbd \u0d86\u0dbb\u0dca\u0dae\u0dd2\u0d9a \u0daf\u0dd2\u0dba\u0dd4\u0dab\u0dd4\u0dc0 \u0dc3\u0dbd\u0dc3\u0dcf \u0daf\u0dd3\u0db8\u0dd9\u0db1\u0dca\u0dba \" )\n( sin_2241_0715400935 \" \u0d89\u0db1\u0dca \u0d85\u0daf\u0dc4\u0dc3\u0dca \u0dc0\u0db1\u0dca\u0db1\u0dda \u0dc0\u0dd2\u0da0\u0dcf\u0dbb\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0dc0\u0dd2\u0db1\u0dd2\u0dc0\u0dd2\u0daf \u0daf\u0dd0\u0d9a\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0dad\u0ddc\u0dbb \u0db6\u0dd0\u0dbd\u0dca\u0db8\u0dba\u0dd2 \" )\n( sin_2241_0817100025 \" \u0d85\u0db4 \u0dba\u0dd4\u0daf\u0dca\u0db0\u0dba\u0dda \u0db4\u0dc5\u0db8\u0dd4 \u0db4\u0dd2\u0dba\u0dc0\u0dbb\u0dda\u0daf\u0dd3\u0db8 \u0db4\u0dbb\u0dcf\u0daf \u0dc0\u0dd3 \u0d85\u0dc0\u0dc3\u0dcf\u0db1\u0dba \" )\n</code></pre> <p>In this format, there are corresponding wav files labelled sin_2241_0329430812.wav etc..</p>"},{"location":"guides/custom/#step-3-install-everyvoice","title":"Step 3: Install EveryVoice","text":"<p>Head over to the install documentation and install EveryVoice</p>"},{"location":"guides/custom/#step-4-run-the-configuration-wizard","title":"Step 4: Run the Configuration Wizard \ud83e\uddd9","text":"<p>Once you have your data, the best thing to do is to run the Configuration Wizard \ud83e\uddd9 which will help you configure a new project. To do that run:</p> <pre><code>everyvoice new-project\n</code></pre> <p>After running the wizard, cd into your newly created directory. Let's call it <code>test</code> for now.</p> <pre><code>cd test\n</code></pre>"},{"location":"guides/custom/#step-5-run-the-preprocessor","title":"Step 5: Run the Preprocessor","text":"<p>Your models need to do a number of preprocessing steps in order to prepare for training. To preprocess everything you need, run the following:</p> <pre><code>everyvoice preprocess config/everyvoice-text-to-spec.yaml\n</code></pre>"},{"location":"guides/custom/#step-6-train-your-vocoder","title":"Step 6: Train your Vocoder","text":"<pre><code>everyvoice train spec-to-wav config/everyvoice-spec-to-wav.yaml\n</code></pre> <p>By default, we run our training with PyTorch Lightning's \"auto\" strategy. But, if you are on a machine where you know the hardware, you can specify it like:</p> <pre><code>everyvoice train spec-to-wav config/everyvoice-spec-to-wav.yaml -d 1 -a gpu\n</code></pre> <p>Which would use the GPU accelerator and specify 1 device/chip.</p>"},{"location":"guides/custom/#step-7-train-your-feature-prediction-network","title":"Step 7: Train your Feature Prediction Network","text":"<p>To generate audio when you train your feature prediction network, you need to add your vocoder checkpoint to the <code>config/everyvoice-text-to-spec.yaml</code></p> <p>At the bottom of that file you'll find a key called vocoder_path. Add the absolute path to your trained vocder (here it would be <code>/path/to/test/logs_and_checkpoints/VocoderExperiment/base/checkpoints/last.ckpt</code> where <code>/path/to</code> would be the actual path to it on your computer.)</p> <p>Once you've replaced the vocoder_path key, you can train your feature prediction network:</p> <pre><code>everyvoice train text-to-spec config/everyvoice-text-to-spec.yaml\n</code></pre>"},{"location":"guides/custom/#step-8-synthesize-speech-in-your-language","title":"Step 8: Synthesize Speech in Your Language!","text":"<p>You can synthesize by pointing the CLI to your trained feature prediction network and passing in the text. You can export to wav, npy, or pt files.</p> <pre><code>everyvoice synthesize text-to-wav logs_and_checkpoints/FeaturePredictionExperiment/base/checkpoints/last.ckpt -t \"\u0db8\u0dd9\u0daf\u0dcf \u0dc3\u0dd0\u0dbb\u0dda \u0dc3\u0dcf\u0d9a\u0da0\u0dca\u0da1\u0dcf\u0dc0\u0d9a\u0dca \u0dc0\u0dd2\u0daf\u0dd2\u0dba\u0da7 \u0db1\u0dd9\u0dc0\u0dd9\u0dba\u0dd2 \u0db1\u0dda\u0daf \u0db4\u0dbd \u0d9a\u0dbb\u0dbd \u0dad\u0dd2\u0dba\u0dd9\u0db1\u0dca\u0db1\u0dd9\" -a gpu -d 1\n</code></pre>"},{"location":"guides/finetune/","title":"How to fine-tune the existing checkpoints","text":""},{"location":"guides/schemas/","title":"How to Setup Code Completion for Schemas in vim","text":"<p>When manually editing EveryVoice's configuration files, it is convenient to have the file checked/validated and to have documentation about each field. The following setup will work for json and yaml configuration files.</p>"},{"location":"guides/schemas/#install-nodejs","title":"Install <code>nodejs</code>","text":"<p>You will need to have a functional <code>npm</code> which is part of <code>nodejs</code>. The schemas will be verified using a node process.</p>"},{"location":"guides/schemas/#install-vim-plug","title":"Install vim-plug","text":"<p>vim-plug:  Minimalist Vim Plugin Manager This will take care of install vim's extensions for us.</p> <pre><code>curl \\\n  --create-dirs \\\n  -fLo ~/.vim/autoload/plug.vim \\\n  https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n</code></pre>"},{"location":"guides/schemas/#augment-your-vimrc","title":"Augment your <code>.vimrc</code>","text":"<p>We want to install Conquer of Completion aka coc We will add a plugins for coc-json and coc-yaml which will be used to handle json and yaml files. Let's also add some key bindings to access <code>coc.nvim</code>'s functionalities. Refer to Example Vim configuration for a more complete example.</p> <pre><code>call plug#begin()\nPlug 'neoclide/coc.nvim', { 'do': 'npm ci' }\n\nlet g:coc_disable_startup_warning = 1\n\n\" Use tab for trigger completion with characters ahead and navigate\n\" NOTE: There's always complete item selected by default, you may want to enable\n\" no select by `\"suggest.noselect\": true` in your configuration file\n\" NOTE: Use command ':verbose imap &lt;tab&gt;' to make sure tab is not mapped by\n\" other plugin before putting this into your config\ninoremap &lt;silent&gt;&lt;expr&gt; &lt;TAB&gt;\n\\ coc#pum#visible() ? coc#pum#next(1) :\n\\ CheckBackspace() ? \"\\&lt;Tab&gt;\" :\n\\ coc#refresh()\ninoremap &lt;expr&gt;&lt;S-TAB&gt; coc#pum#visible() ? coc#pum#prev(1) : \"\\&lt;C-h&gt;\"\n\n\" Make &lt;CR&gt; to accept selected completion item or notify coc.nvim to format\n\" &lt;C-g&gt;u breaks current undo, please make your own choice\ninoremap &lt;silent&gt;&lt;expr&gt; &lt;CR&gt; coc#pum#visible() ? coc#pum#confirm()\n\\: \"\\&lt;C-g&gt;u\\&lt;CR&gt;\\&lt;c-r&gt;=coc#on_enter()\\&lt;CR&gt;\"\n\nfunction! CheckBackspace() abort\n  let col = col('.') - 1\n  return !col || getline('.')[col - 1]  =~# '\\s'\nendfunction\n\n\" Use &lt;c-space&gt; to trigger completion\nif has('nvim')\n  inoremap &lt;silent&gt;&lt;expr&gt; &lt;c-space&gt; coc#refresh()\nelse\n  inoremap &lt;silent&gt;&lt;expr&gt; &lt;c-@&gt; coc#refresh()\nendif\n\n\" Use `[g` and `]g` to navigate diagnostics\n\" Use `:CocDiagnostics` to get all diagnostics of current buffer in location list\nnmap &lt;silent&gt; [g &lt;Plug&gt;(coc-diagnostic-prev)\nnmap &lt;silent&gt; ]g &lt;Plug&gt;(coc-diagnostic-next)\n\n\" GoTo code navigation\nnmap &lt;silent&gt; gd &lt;Plug&gt;(coc-definition)\nnmap &lt;silent&gt; gy &lt;Plug&gt;(coc-type-definition)\nnmap &lt;silent&gt; gi &lt;Plug&gt;(coc-implementation)\nnmap &lt;silent&gt; gr &lt;Plug&gt;(coc-references)\n\n\" Use K to show documentation in preview window\nnnoremap &lt;silent&gt; K :call ShowDocumentation()&lt;CR&gt;\n\nfunction! ShowDocumentation()\n  if CocAction('hasProvider', 'hover')\n    call CocActionAsync('doHover')\n  else\n    call feedkeys('K', 'in')\n  endif\nendfunction\n\n\" Highlight the symbol and its references when holding the cursor\nautocmd CursorHold * silent call CocActionAsync('highlight')\n\n\" Symbol renaming\nnmap &lt;leader&gt;rn &lt;Plug&gt;(coc-rename)\n\n\" Formatting selected code\nxmap &lt;leader&gt;F  &lt;Plug&gt;(coc-format-selected)\nnmap &lt;leader&gt;F  &lt;Plug&gt;(coc-format-selected)\n\naugroup mygroup\nautocmd!\n\" Setup formatexpr specified filetype(s)\nautocmd FileType typescript,json setl formatexpr=CocAction('formatSelected')\n\" Update signature help on jump placeholder\nautocmd User CocJumpPlaceholder call CocActionAsync('showSignatureHelp')\naugroup end\n\n\" Applying code actions to the selected code block\n\" Example: `&lt;leader&gt;aap` for current paragraph\nxmap &lt;leader&gt;a  &lt;Plug&gt;(coc-codeaction-selected)\nnmap &lt;leader&gt;a  &lt;Plug&gt;(coc-codeaction-selected)\n\n\" Remap keys for applying code actions at the cursor position\nnmap &lt;leader&gt;ac  &lt;Plug&gt;(coc-codeaction-cursor)\n\" Remap keys for apply code actions affect whole buffer\nnmap &lt;leader&gt;as  &lt;Plug&gt;(coc-codeaction-source)\n\" Apply the most preferred quickfix action to fix diagnostic on the current line\nnmap &lt;leader&gt;qf  &lt;Plug&gt;(coc-fix-current)\n\n\" Remap keys for applying refactor code actions\nnmap &lt;silent&gt; &lt;leader&gt;re &lt;Plug&gt;(coc-codeaction-refactor)\nxmap &lt;silent&gt; &lt;leader&gt;r  &lt;Plug&gt;(coc-codeaction-refactor-selected)\nnmap &lt;silent&gt; &lt;leader&gt;r  &lt;Plug&gt;(coc-codeaction-refactor-selected)\n\n\" Run the Code Lens action on the current line\nnmap &lt;leader&gt;cl  &lt;Plug&gt;(coc-codelens-action)\n\n\" Map function and class text objects\n\" NOTE: Requires 'textDocument.documentSymbol' support from the language server\nxmap if &lt;Plug&gt;(coc-funcobj-i)\nomap if &lt;Plug&gt;(coc-funcobj-i)\nxmap af &lt;Plug&gt;(coc-funcobj-a)\nomap af &lt;Plug&gt;(coc-funcobj-a)\nxmap ic &lt;Plug&gt;(coc-classobj-i)\nomap ic &lt;Plug&gt;(coc-classobj-i)\nxmap ac &lt;Plug&gt;(coc-classobj-a)\nomap ac &lt;Plug&gt;(coc-classobj-a)\n\n\" Remap &lt;C-f&gt; and &lt;C-b&gt; to scroll float windows/popups\nif has('nvim-0.4.0') || has('patch-8.2.0750')\nnnoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-f&gt; coc#float#has_scroll() ? coc#float#scroll(1) : \"\\&lt;C-f&gt;\"\nnnoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-b&gt; coc#float#has_scroll() ? coc#float#scroll(0) : \"\\&lt;C-b&gt;\"\ninoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-f&gt; coc#float#has_scroll() ? \"\\&lt;c-r&gt;=coc#float#scroll(1)\\&lt;cr&gt;\" : \"\\&lt;Right&gt;\"\ninoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-b&gt; coc#float#has_scroll() ? \"\\&lt;c-r&gt;=coc#float#scroll(0)\\&lt;cr&gt;\" : \"\\&lt;Left&gt;\"\nvnoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-f&gt; coc#float#has_scroll() ? coc#float#scroll(1) : \"\\&lt;C-f&gt;\"\nvnoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-b&gt; coc#float#has_scroll() ? coc#float#scroll(0) : \"\\&lt;C-b&gt;\"\nendif\n\n\" Use CTRL-S for selections ranges\n\" Requires 'textDocument/selectionRange' support of language server\nnmap &lt;silent&gt; &lt;C-s&gt; &lt;Plug&gt;(coc-range-select)\nxmap &lt;silent&gt; &lt;C-s&gt; &lt;Plug&gt;(coc-range-select)\n\n\" Add `:Format` command to format current buffer\ncommand! -nargs=0 Format :call CocActionAsync('format')\n\n\" Add `:Fold` command to fold current buffer\ncommand! -nargs=? Fold :call     CocAction('fold', &lt;f-args&gt;)\n\n\" Add `:OR` command for organize imports of the current buffer\ncommand! -nargs=0 OR   :call     CocActionAsync('runCommand', 'editor.action.organizeImport')\n\n\" Add (Neo)Vim's native statusline support\n\" NOTE: Please see `:h coc-status` for integrations with external plugins that\n\" provide custom statusline: lightline.vim, vim-airline\nset statusline^=%{coc#status()}%{get(b:,'coc_current_function','')}\n\n\" TODO Space is our Leader, this might interfer with the following:\n\" Mappings for CoCList\n\" Show all diagnostics\nnnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;a  :&lt;C-u&gt;CocList diagnostics&lt;cr&gt;\n\" Manage extensions\nnnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;e  :&lt;C-u&gt;CocList extensions&lt;cr&gt;\n\" Show commands\nnnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;c  :&lt;C-u&gt;CocList commands&lt;cr&gt;\n\" Find symbol of current document\nnnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;o  :&lt;C-u&gt;CocList outline&lt;cr&gt;\n\" Search workspace symbols\nnnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;s  :&lt;C-u&gt;CocList -I symbols&lt;cr&gt;\n\" Do default action for next item\nnnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;j  :&lt;C-u&gt;CocNext&lt;CR&gt;\n\" Do default action for previous item\nnnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;k  :&lt;C-u&gt;CocPrev&lt;CR&gt;\n\" Resume latest coc list\nnnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;p  :&lt;C-u&gt;CocListResume&lt;CR&gt;\ncall plug#end()\n</code></pre>"},{"location":"guides/schemas/#install-the-new-plugins","title":"Install the New Plugins","text":"<p>Plugins don't automatically install themself thus you have to run the following command to install them. Start <code>vim</code> then do</p> <pre><code>vim +PlugInstall \"+:CocInstall coc-json\" \"+:CocInstall coc-yaml\" +:qall\n</code></pre>"},{"location":"guides/schemas/#compile-cocnvim","title":"Compile coc.nvim","text":"<p>Once your plugins are installed, you will need to compile coc.</p> <pre><code>cd ~/.vim/plugged/coc.nvim\nnpm ci\n</code></pre>"},{"location":"guides/schemas/#create-coc-settingsjson","title":"Create Coc-settings.json","text":"<p>Start <code>vim</code> and run the command <code>:CocConfig</code> to edit where your everyvoice schemas are located. The following example assumes that you have clone EveryVoice into <code>~/git/EveryVoice</code>. Make the proper modifications to match where you have cloned EveryVoice. Also note that you have to change <code>/home/username</code> with your own username in the yaml section.</p> <pre><code>{\n  \"json.schemas\": [\n    {\n      \"url\": \"file://${userHome}/git/EveryVoice/everyvoice/.schema/everyvoice-aligner-schema-0.1.json\",\n      \"fileMatch\": [\n        \"everyvoice-aligner.json\"\n      ]\n    },\n    {\n      \"url\": \"file://${userHome}/git/EveryVoice/everyvoice/.schema/everyvoice-shared-data-schema-0.1.json\",\n      \"fileMatch\": [\n        \"everyvoice-shared-data.json\"\n      ]\n    },\n    {\n      \"url\": \"file://${userHome}/git/EveryVoice/everyvoice/.schema/everyvoice-shared-text-schema-0.1.json\",\n      \"fileMatch\": [\n        \"everyvoice-shared-text.json\"\n      ]\n    },\n    {\n      \"url\": \"file://${userHome}/git/EveryVoice/everyvoice/.schema/everyvoice-spec-to-wav-schema-0.1.json\",\n      \"fileMatch\": [\n        \"everyvoice-spec-to-wav.json\"\n      ]\n    },\n    {\n      \"url\": \"file://${userHome}/git/EveryVoice/everyvoice/.schema/everyvoice-text-to-spec-schema-0.1.json\",\n      \"fileMatch\": [\n        \"everyvoice-text-to-spec.json\"\n      ]\n    },\n    {\n      \"url\": \"file://${userHome}/git/EveryVoice/everyvoice/.schema/everyvoice-text-to-wav-schema-0.1.json\",\n      \"fileMatch\": [\n        \"everyvoice-text-to-wav.json\"\n      ]\n    }\n  ],\n  \"yaml.schemas\": {\n    \"file://home/username/git/EveryVoice/everyvoice/.schema/everyvoice-aligner-schema-0.1.json\": [\n      \"everyvoice-aligner.yaml\"\n    ],\n    \"file://home/username/git/EveryVoice/everyvoice/.schema/everyvoice-shared-data-schema-0.1.json\": [\n      \"everyvoice-shared-data.yaml\"\n    ],\n    \"file://home/username/git/EveryVoice/everyvoice/.schema/everyvoice-shared-text-schema-0.1.json\": [\n      \"everyvoice-shared-text.yaml\"\n    ],\n    \"file://home/username/git/EveryVoice/everyvoice/.schema/everyvoice-spec-to-wav-schema-0.1.json\": [\n      \"everyvoice-spec-to-wav.yaml\"\n    ],\n    \"file://home/username/git/EveryVoice/everyvoice/.schema/everyvoice-text-to-spec-schema-0.1.json\": [\n      \"everyvoice-text-to-spec.yaml\"\n    ],\n    \"file://home/username/git/EveryVoice/everyvoice/.schema/everyvoice-text-to-wav-schema-0.1.json\": [\n      \"everyvoice-text-to-wav.yaml\"\n    ]\n  }\n}\n</code></pre>"},{"location":"guides/schemas/#usage","title":"Usage","text":"<p>Once everything is installed, start editing a new or existing EveryVoice configuration.</p> <pre><code>vim everyvoice-shared-data.json\n</code></pre> <p>Then use <code>CTRL+&lt;space&gt;</code> to trigger completion.</p>"},{"location":"reference/","title":"Reference","text":"<p>Here is where you will find information about the various models implemented in EveryVoice. This section will include some fairly technical details. If you just want to build a model using default settings and configurations, please visit the documentation on the guides</p>"},{"location":"reference/aligner/","title":"Aligner","text":"<p>Source Code: https://github.com/roedoejet/DeepForcedAligner</p> <p>The Aligner module is for training a model of alignment between your data's text and speech. In other implementations, this is handled by a 3rd party aligner, like the Montreal Forced Aligner (MFA). Similar to IMS-Toucan, we have implemented our own based on DeepForcedAligner to ensure that the way our model processes text and audio remains consistent between the aligner and the other models.</p>"},{"location":"reference/aligner/#command-line-interface","title":"Command Line Interface","text":"<pre><code>.. click:: everyvoice.cli:CLICK_APP\n    :prog: everyvoice\n    :nested: full\n    :commands: dfa\n</code></pre>"},{"location":"reference/aligner/#configuration","title":"Configuration","text":""},{"location":"reference/aligner/#main-configuration","title":"Main Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.aligner.DeepForcedAligner.dfaligner.config.DFAlignerConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/aligner/#training-configuration","title":"Training Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.aligner.DeepForcedAligner.dfaligner.config.DFAlignerTrainingConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/aligner/#model-configuration","title":"Model Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.aligner.DeepForcedAligner.dfaligner.config.DFAlignerModelConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Each model has a statically typed configuration model. Each configuration has default settings that will be instantiated when the model is instantiated. To create a default preprocessing configuration for example you would:</p> <pre><code>from everyvoice.config.preprocessing_config import PreprocessingConfig\n\npreprocessing_config = PreprocessingConfig()\n</code></pre> <p>Static typing means that if someone accidentally enters an integer that the configuration expects should be a float or some other trivial typing difference, it is corrected when the configuration is instantiated, and doesn't produce downstream runtime errors. It also means that intellisense is available in your code editor when working with a configuration class.</p>"},{"location":"reference/configuration/#configuration-wizard","title":"Configuration Wizard \ud83e\uddd9\u200d\u2640\ufe0f","text":"<p>This section of the documentation is meant for technical explanations and reference documentation - if you're just looking to get started, have a look at the {ref}<code>guides</code> section instead to learn about how the Configuration Wizard \ud83e\uddd9\u200d\u2640\ufe0f can help you configure everything for your dataset.</p> <p>Here is the reference documentation for the Configuration Wizard \ud83e\uddd9\u200d\u2640\ufe0f</p>"},{"location":"reference/configuration/#sharing-configurations","title":"Sharing Configurations","text":"<p>The Text and Preprocessing configurations should only be defined once per dataset and shared between your models to ensure each model makes the same assumptions about your data. To achieve that, each model configuration can also be defined as a path to a configuration file. So, a configuration for an aligner model that uses separately defined text and audio preprocessing configurations might look like this:</p> <pre><code>model:\n    lstm_dim: 512\n    conv_dim: 512\n    ...\ntraining:\n    batch_size: 32\n    ...\npreprocessing: \"./config/default/everyvoice-shared-data.yaml\"\ntext: \"./config/default/everyvoice-shared-text.yaml\"\n</code></pre>"},{"location":"reference/configuration/#serialization","title":"Serialization","text":"<p>By default configuration objects are serialized as dictionaries, which works as expected with integers, floats, lists, booleans, dicts etc. But there are some cases where you need to specify a Callable in your configuration. For example the {ref}<code>TextConfig</code> has a <code>cleaners</code> field that takes a list of Callables to apply in order to raw text. By default, these functions turn raw text to lowercase, collapse whitespace, and normalize using Unicode NFC normalization. In Python, we could instantiate this by passing the callables directly like so:</p> <pre><code>from everyvoice.config.text_config import TextConfig\nfrom everyvoice.utils import collapse_whitespace, lower, nfc_normalize\n\ntext_config = TextConfig(cleaners=[lower, collapse_whitespace, nfc_normalize])\n</code></pre> <p>But, for yaml or json configuration, we need to serialize these functions. To do so, EveryVoice will turn each callable into module dot-notation. That is, your configuration will look like this in yaml:</p> <pre><code>cleaners:\n    - everyvoice.utils.lower\n    - everyvoice.utils.collapse_whitespace\n    - everyvoice.utils.nfc_normalize\n</code></pre> <p>This will then be de-serialized upon instantiation of your configuration.</p>"},{"location":"reference/configuration/#text-configuration","title":"Text Configuration","text":"<p>The TextConfig is where you define the symbol set for your data and any cleaners used to clean your raw text into the text needed for your data. You can share the TextConfig with any models that need it and only need one text configuration per dataset (and possibly only per language).</p> <p>Note</p> <p>Only the aligner, feature_prediction, and e2e models need text configuration. The vocoder is trained without text and does not need a TextConfig. For more information on this, see the background section.</p>"},{"location":"reference/configuration/#textconfig","title":"TextConfig","text":""},{"location":"reference/configuration/#everyvoice.config.text_config.TextConfig","title":"<code>everyvoice.config.text_config.TextConfig</code>","text":"<p>             Bases: <code>ConfigModel</code></p> Source code in <code>everyvoice/config/text_config.py</code> <pre><code>class TextConfig(ConfigModel):\n    symbols: Symbols = Field(default_factory=Symbols)  # type: ignore\n    to_replace: Dict[str, str] = {}  # Happens before cleaners\n    cleaners: List[PossiblySerializedCallable] = [\n        lower,\n        collapse_whitespace,\n        nfc_normalize,\n    ]\n</code></pre> <code>cleaners: List[PossiblySerializedCallable] = [lower, collapse_whitespace, nfc_normalize]</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>symbols: Symbols = Field(default_factory=Symbols)</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <code>to_replace: Dict[str, str] = {}</code> <code>class-attribute</code> <code>instance-attribute</code> \u00b6"},{"location":"reference/configuration/#symbols","title":"Symbols","text":"<pre><code>.. autopydantic_settings:: everyvoice.config.text_config.Symbols\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/configuration/#preprocessing-configuration","title":"Preprocessing Configuration","text":""},{"location":"reference/e2e/","title":"E2E","text":""},{"location":"reference/e2e/#command-line-interface","title":"Command Line Interface","text":"<pre><code>.. click:: everyvoice.cli:CLICK_APP\n    :prog: everyvoice\n    :nested: full\n    :commands: e2e\n</code></pre>"},{"location":"reference/e2e/#configuration","title":"Configuration","text":""},{"location":"reference/e2e/#main-configuration","title":"Main Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.e2e.config.EveryVoiceConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/feature_prediction/","title":"Feature Prediction","text":""},{"location":"reference/feature_prediction/#fastspeech2","title":"FastSpeech2","text":"<p>Source Code: https://github.com/roedoejet/FastSpeech2_lightning</p> <p>The Feature Prediction module is for training a model of alignment between your data's text and speech. In other implementations, this is handled by a 3rd party aligner, like the Montreal Forced Aligner (MFA). Similar to IMS-Toucan, we have implemented our own based on DeepForcedAligner to ensure that the way our model processes text and audio remains consistent between the aligner and the other models.</p>"},{"location":"reference/feature_prediction/#command-line-interface","title":"Command Line Interface","text":"<pre><code>.. click:: everyvoice.cli:CLICK_APP\n    :prog: everyvoice\n    :nested: full\n    :commands: fs2\n</code></pre>"},{"location":"reference/feature_prediction/#configuration","title":"Configuration","text":""},{"location":"reference/feature_prediction/#main-configuration","title":"Main Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.feature_prediction.FastSpeech2_lightning.fs2.config.FastSpeech2Config\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/feature_prediction/#training-configuration","title":"Training Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.feature_prediction.FastSpeech2_lightning.fs2.config.FastSpeech2TrainingConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/feature_prediction/#model-configuration","title":"Model Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.feature_prediction.FastSpeech2_lightning.fs2.config.FastSpeech2ModelConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/vocoder/","title":"Vocoder","text":""},{"location":"reference/vocoder/#hifiganistft-net","title":"HiFiGAN/iSTFT-Net","text":"<p>Source Code: https://github.com/roedoejet/HiFiGAN_iSTFT_lightning</p> <p>This vocoder is based on the HiFiGAN/iSTFT-Net neural vocoders.</p>"},{"location":"reference/vocoder/#command-line-interface","title":"Command Line Interface","text":"<pre><code>.. click:: everyvoice.cli:CLICK_APP\n    :prog: everyvoice\n    :nested: full\n    :commands: hifigan\n</code></pre>"},{"location":"reference/vocoder/#configuration","title":"Configuration","text":""},{"location":"reference/vocoder/#main-configuration","title":"Main Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.vocoder.HiFiGAN_iSTFT_lightning.hfgl.config.HiFiGANConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/vocoder/#training-configuration","title":"Training Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.vocoder.HiFiGAN_iSTFT_lightning.hfgl.config.HiFiGANTrainingConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"},{"location":"reference/vocoder/#model-configuration","title":"Model Configuration","text":"<pre><code>.. autopydantic_settings:: everyvoice.model.vocoder.HiFiGAN_iSTFT_lightning.hfgl.config.HiFiGANModelConfig\n    :settings-show-json: True\n    :settings-show-config-member: False\n    :settings-show-config-summary: False\n    :settings-show-validator-members: True\n    :settings-show-validator-summary: True\n    :field-list-validators: True\n</code></pre>"}]}